
dashboadtags:
    - LLM,Dev
    - LLM,OOB
    - LLM,Dev,DeepSpeed
    - LLM,OOB,DeepSpeed
WorkLoad:
    GPTJ-PyTorch-Dev:
        test_cases:
            - pkm
            - accuracy
        special_args:
            MODEL_NAME:
                - EleutherAI/gpt-j-6b: /mnt/nfs/huggingface/hub/gptj-6b
        set_args:
            STEPS: 20
            USE_DEEPSPEED: True
            RANK_USE: all
            ONEDNN_VERBOSE: 0
            PRECISION:
                - amx_bfloat16
                - amx_int8
            INPUT_TOKENS:
                - 32
                - 1024
            OUTPUT_TOKENS:
                - 32
    # Llama2-PyTorch-Dev:
    #     test_cases:
    #         - pkm
    #         - accuracy
    #     special_args:
    #         MODEL_NAME:
    #             - meta-llama/Llama-2-7b-chat-hf: /mnt/nfs/huggingface/hub/llama2-7b
    #             # - meta-llama/Llama-2-13b-chat-hf: /opt/dataset/llama2/13b
    #     set_args:
    #         STEPS: 20
    #         USE_DEEPSPEED: True
    #         RANK_USE: all
    #         ONEDNN_VERBOSE: 0
    #         PRECISION:
    #             - amx_bfloat16
    #             - amx_int8
    #         INPUT_TOKENS:
    #             - 32
    #             - 1024
    #         OUTPUT_TOKENS:
    #             - 32
    OPT-PyTorch-Dev:
        test_cases:
            - pkm
            - accuracy
        special_args:
            MODEL_NAME:
                - facebook/opt-1.3b: /opt/dataset/opt/1b3
        set_args:
            STEPS: 20
            USE_DEEPSPEED: True
            ONEDNN_VERBOSE: 0 
            PRECISION:
                - amx_bfloat16
                - amx_int8
            INPUT_TOKENS:
                - 32
                - 1024
            OUTPUT_TOKENS:
                - 32
    # GPTJ-PyTorch-OOB:
    #     test_cases:
    #         - pkm
    #     special_args:
    #         MODEL_NAME:
    #             - EleutherAI/gpt-j-6b: /mnt/nfs_share/huggingface/hub/gptj-6b
    #     set_args:
    #         STEPS: 20
    #         USE_DEEPSPEED: True
    #         ONEDNN_VERBOSE: 1
    #         PRECISION:
    #             - bfloat16
    #         INPUT_TOKENS:
    #             - 32
    #             # - 1024
    #         OUTPUT_TOKENS:
    #             # - 1
    #             - 32
    # Llama2-PyTorch-OOB:
    #     test_cases:
    #         - pkm
    #     special_args:
    #         MODEL_NAME:
    #             - meta-llama/Llama-2-7b-chat-hf: /mnt/nfs_share/huggingface/hub/llama2-7b
    #             # - meta-llama/Llama-2-13b-chat-hf: /opt/dataset/llama2/13b
    #     set_args:
    #         STEPS: 20
    #         USE_DEEPSPEED: True
    #         ONEDNN_VERBOSE: 1
    #         PRECISION:
    #             - bfloat16
    #         INPUT_TOKENS:
    #             - 32
    #             # - 1024
    #         OUTPUT_TOKENS:
    #             # - 1
    #             - 32
    # Bloom-PyTorch-Dev:
    #     test_cases:
    #         - pkm
    #     special_args:
    #         MODEL_NAME:
    #             - bigscience/bloom-7b1: /mnt/nfs_share/huggingface/hub/
    #     set_args:
    #         STEPS: 20
    #         USE_DEEPSPEED: True
    #         ONEDNN_VERBOSE: 1 
    #         PRECISION:
    #             - amx_bfloat16
    #             # - amx_int8
    #         INPUT_TOKENS:
    #             - 32
    #             # - 1024
    #         OUTPUT_TOKENS:
    #             # - 1
    #             - 32
    